{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118B - Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be utilizing a stock market dataset from Kaggle that contains the historical daily prices of Nasdaq-traded stocks and ETFs. Our goal is to implement unsupervised machine learning to optimize a given stock portfolio based off various risk-return profiles. Specifically, we will be utilizing unsupervised machine learning techniques such as clustering or dimensionality reduction to filter out the irrelevant information. We plan to first clean the data, implement machine learning techniques, and then compare the results to a traditional stock portfolio to gauge our results. Libraries such as numpy and pandas can be used as tools to achieve our desired results. Some specific metrics we will use are the Sharpe Ratio and the Compound Annual Growth Rate - we will delve into these metrics more deeply later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "Hopefully your team is at least this good. Obviously you should replace these with your names.\n",
    "\n",
    "- Griffin Barros-King\n",
    "- Alex Vo\n",
    "- Xinyi Zhang\n",
    "- Ryan Wong\n",
    "- Franz Beckenbaur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "Unsupervised machine learning presents a viable approach to optimizing stock portfolios by analyzing risk-return profiles without explicit guidance or predefined labels. The research aims to explore the effectiveness of leveraging unsupervised learning algorithms to categorize stocks based on the risk-return variability. Using the historical stock market data such as daily prices, volatility, etc. and financial ratios, we can identify the correlations that can inform portfolio optimization. We can apply clustering techniques and dimensionality reduction methods, we can aim to group stocks into distinct categories that reflect the risk-return dynamics. The performance and success of this unsupervised machine learning approach will be evaluated based on the portfolios overall stability compared to traditional portfolio optimization methods. This study also aims to provide insights into the applicability of unsupervised learning in financial portfolio management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Stock trading involves buying and selling financial instruments such as stocks, bonds, or derivatives with the goal of generating profits from fluctuations in their prices. Stock portfolios reffer to the collections of such financial assets held by investors, which are carefully constructed to achieve specific investment objectives, such as maximizing returns, minimizing risk, or achieving a balance between the two. Unsupervised machine learning techniques, such as clustering or dimensionality reduction, can assist in portfolio construction by grouping assets with similar risk-return profiles or reducing the dimensionality of the feature space while perserving relevant information. This can aid in the creation of diversified portfolios that balance risk and return effectively. The application of unsupervised machine learning methods on optimizing stock portfolios have been gaining considerable attention due to its limitless potential to outperform traditional statistical methods in portfolio management by means of pattern recognition, anomaly detection, and feature extraction from historical stock market data that all falls under unsupervised machine learning techniques that recognizes shifts in patterns or sudden changes in data to provide valuable signals for traders and portfolio managers to take appropriate action. Prior research that supports the use of machine learning methods that utilizes historical stock market data to analyze and optimize stock portfolios have been conducted. The application of the clustering method using K-Means helped reduce loan risk categories to minimize business losses <a name=\"ieee\"></a>[<sup>[4]</sup>](#ieeenote). It proved to have high accuracy in predicting loan risk to achieve overall refined stock portfolios. Unsupervised machine learning methods can also extract meaningful features from raw financial data, such as technical indicators, sentiment analysis from news articles, or fundamental ratios from financial statements. <a name=\"oleh\"></a>[<sup>[5]</sup>](#olehnote). These extracted features can be used as inputs to predictive models for forecasting stock prices or estimating future returns. by leveraging these methods, investors can gain deeper insights into financial markets, enhance their trading strategies, and ultimately achieve better risk-adjusted returns in their portfolios compared to relying only on traditional portfolio optimization methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "We are optimizing stock portfolios based off certain risk-return investor preferences. The problem is that a given investor does not know how to optimize their portfolio because there is a plethora of financial advice on the internet that encourages different investing styles. One machine learning relevant potential solution is utilizing a clustering technique such as K-means or the Gaussian Mixture Model to group stocks based off their risk return profiles. The problem is measurable because we will be comparing our data against the Sharpe ratio metric and the Compound Annual Growth Rate. This experiment can be easily reproduced because of the large sample size of our dataset. Our dataset contains all of the stock market data up until 2020. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "1. \n",
    "- The dataset we are using is a stock market dataset that contains some price and volume metrics for every stock and etf in the NASDAQ exchange for every day from 11/08/1999 to 04/01/2020.\n",
    "- Link to dataset: https://www.kaggle.com/datasets/jacksoncrow/stock-market-dataset\n",
    "- The dataset has 7 variables and 7450 observations(per stock/etf, 5884 stocks, 2165 etfs)\n",
    "- Each observation consists of the date along with the opening price, closing price, high price, low price, adjusted closing price, and volume traded on that day.\n",
    "- For looking at volatility in each stock we will likely want to look at the differences between the high and low prices to see how much a stock can change in a given day.\n",
    "We will also want to look at, over a given set of observations. see how the opening and closing prices change. Looking at if they go up consistently over time to imply a good return.\n",
    "- The dataset is already relatively clean. The one thing we may want to do is normalize the prices, so we can look at the volatility and returns in terms of percent change easier between stocks.\n",
    "We will likely want to adjust the names of the stocks from their tickers to the official names for readability.\n",
    "If you don't yet know what your dataset(s) will be, you should describe what you desire in terms of the above bullets.\n",
    "2. \n",
    "- Another dataset we may want would be similar historical data but with multiple observations per day. This would be more useful in analyzing volatility on a shorter term basis.\n",
    "- This would contain the same variables with more observations as it would contain multiple observations per day.\n",
    "- This would also allow us to look at more interesting volatility metrics other than just daily highs and lows. like average price change per hour(or per however often the intra-day observations take place)\n",
    "3. \n",
    "- A dataset that could be useful for non-analysis purposes would be a list of each stock and etf on the nasdaq exchange with its ticker name and its official name.\n",
    "- this would consist of 2 variables and 8049 observations (one for each stock/etf)\n",
    "- The variables would simply be the ticker name and official name for each stock/etf\n",
    "- This dataset would ideally already be clean and would be use to clean up our other datasets and improve readability in any tables or graphs that we present in the final project.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "In order to address problem of optimizing stock portfolios using unsupervised machine learning techniques, we propose the following solution:\n",
    "\n",
    "Data preprocessing: \n",
    "\n",
    "- cleaning dataset: remove unneccessary data and handling missing values to keep data consistency.\n",
    "- Normalize the prices: Convert the prices into percentage changes to facilitate easy comparison of volatility and returns across different stocks.\n",
    "- calculate additional features about stock protfoloios. For example: price volatility, average price change. \n",
    "\n",
    "Unsupervised Learning:\n",
    "\n",
    "- Utilize clustering techniques such as K-Means or Gaussian Mixture Models to categorize stocks based on their risk-return profiles.\n",
    "- Implement dimensionality reduction methods like Principal Component Analysis (PCA) to reduce the feature space while preserving relevant information.\n",
    "\n",
    "Benchmark Model:\n",
    "\n",
    "- Compare the performance of our unsupervised learning-based approach with traditional portfolio optimization method\n",
    "\n",
    "Portfolio Optimization\n",
    "\n",
    "- Construct portfolios by selecting stocks from each cluster to achieve desired risk-return profiles.\n",
    "- Evaluate portfolio performance using metrics such as Sharpe ratio, cumulative returns, and portfolio volatility.\n",
    "\n",
    "Implementation:\n",
    "- Utilize Python libraries such as pandas, NumPy, and scikit-learn for data preprocessing, feature engineering, and model implementation.\n",
    "- For clustering and dimensionality reduction, use scikit-learn's implementations of K-Means and PCA.\n",
    "- Visualize results using matplotlib or seaborn to provide insights into portfolio composition and performance.\n",
    "\n",
    "Testing:\n",
    "- Split the dataset into training and testing sets to evaluate model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "Propose at least one evaluation metric that can be used to quantify the performance of both the benchmark model and the solution model. The evaluation metric(s) you propose should be appropriate given the context of the data, the problem statement, and the intended solution. Describe how the evaluation metric(s) are derived and provide an example of their mathematical representations (if applicable). Complex evaluation metrics should be clearly defined and quantifiable (can be expressed in mathematical or logical terms).\n",
    "\n",
    "Total return and Compound Annual Growth Rate (CAGR):\n",
    "\n",
    "- $CAGR = \\left( \\frac{{P_f}}{{P_i}} \\right)^{\\frac{1}{n}} - 1$\n",
    "where P_i is the initial investment value, P_f is the final investment value, n is the number of years in the investment period.\n",
    "\n",
    "Sharpe ratio:\n",
    "\n",
    "- $Sharpe Ratio = \\frac{{R_p - R_f}}{{\\sigma_p}}$\n",
    "where R_p is the average return of the portfolio, R-f is the risk-free rate of return, and signma_p is the standard deviation of the portfolio's excess return.\n",
    "\n",
    "For the benchmark model and the solution model, we can calculate the Sharpe ratio for each portfolio constructed using their respective optimization methods. We then compare the Sharpe ratios to assess which model generates portfolios with superior risk-adjusted returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main ethical issues that we believe is present is a strong bias towards American markets. Since we only have data from the NASDAQ exchange, any beneficial information or model we make may not be very effective in foreign markets. This could lead to an unfair benefit towards people who invest in American markets versus those who don't or can't. This could be mitigated by finding datasets that include information from multiple markets other than the NASDAQ. We could also just test a few other markets and see if the conclusions we find in the American markets carry over to those ones. If so, we can possibly show that the benefit of any model developed would carry over to other markets.\n",
    "\n",
    "Another potential issue is the unfairness of those who have the technology to run whatever model we develop versus those who don't. While the world has seen a great increase in access to technology. There are still great differences in the quality of the technology people can access. Seeing as the models we will develop will run based off of large datasets and may get quite complex. It is possible that our model would be more beneficial to those who can run similar simulations quicker on better technology. This could create an unfair financial advantage to those who are more likely to already be financially better off, based on the fact that they have access to better technology.\n",
    "\n",
    "Another potential issue is if the model is effective enough in predicting and managing stocks. It could lead to a loss in jobs of those who work in the financial sector. A way to mitigate both this issue and the previously addressed issue would be to restrict the use of our model to research purposes. therefore preventing any market advantages from being gained by those who would unfairly benefit from the model.\n",
    "\n",
    "There are no issues with data privacy for our datasets. This is because all the information we use is publicly available and must be by law. There is also no PII in the dataset so that is also beneficial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Mandatory weekly meetings on Tuesdays\n",
    "* Main form of communication through Discord\n",
    "* Even split of workload/parts\n",
    "* Complete assigned parts at least a day before official deadline for any group discussion/review\n",
    "* Midweek group check-ins to see how everyone is doing\n",
    "* If any conflicts arise or any help is needed, message groupchat on Discord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace this with something meaningful that is appropriate for your needs. It doesn't have to be something that fits this format.  It doesn't have to be set in stone... \"no battle plan survives contact with the enemy\". But you need a battle plan nonetheless, and you need to keep it updated so you understand what you are trying to accomplish, who's responsible for what, and what the expected due dates are for each item.\n",
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 1/20  |  1 PM |  Brainstorm topics/questions (all)  | Determine best form of communication; Discuss and decide on final project topic; discuss hypothesis; begin background research | \n",
    "| 1/26  |  10 AM |  Do background research on topic (Pelé) | Discuss ideal dataset(s) and ethics; draft project proposal | \n",
    "| 2/1  | 10 AM  | Edit, finalize, and submit proposal; Search for datasets (Beckenbaur)  | Discuss Wrangling and possible analytical approaches; Assign group members to lead each specific part   |\n",
    "| 2/14  | 6 PM  | Import & Wrangle Data ,do some EDA (Maradonna) | Review/Edit wrangling/EDA; Discuss Analysis Plan   |\n",
    "| 2/23  | 12 PM  | Finalize wrangling/EDA; Begin programming for project (Cruyff) | Discuss/edit project code; Complete project |\n",
    "| 3/13  | 12 PM  | Complete analysis; Draft results/conclusion/discussion (Carlos)| Discuss/edit full project |\n",
    "| 3/19  | Before 11:59 PM  | NA | Turn in Final Project  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "\n",
    "<a name=\"ieeenote\"></a>1.[^](#ieee): S. H. Sudjono, F. H. Adrian, C. A. Sunarya, G. F. Ariyanto and N. T. M. Sagala, \"Comparison of Different Machine Learning Algorithms for Predicting Loan Risk Categories,\" 2023 International Conference on Computer Science, Information Technology and Engineering (ICCoSITE), Jakarta, Indonesia, 2023, pp. 773-778, doi: 10.1109/ICCoSITE57641.2023.10127758. keywords: {Machine learning algorithms;Computational modeling;Clustering algorithms;Companies;Predictive models;Boosting;Prediction algorithms;kmeans clustering;boosting algorithm;loan risk;predictive model},\n",
    "\n",
    "<a name=\"olehnote\"></a>2.[^](#oleh): Oleh Onyshchak. (2020). Stock Market Dataset [Data set]. Kaggle. https://doi.org/10.34740/KAGGLE/DSV/1054465\n",
    "\n",
    "\n",
    "Template:\n",
    "\n",
    "<a name=\"lorenznote\"></a>3.[^](#lorenz): Lorenz, T. (9 Dec 2021) Birds Aren’t Real, or Are They? Inside a Gen Z Conspiracy Theory. *The New York Times*. https://www.nytimes.com/2021/12/09/technology/birds-arent-real-gen-z-misinformation.html<br> \n",
    "<a name=\"admonishnote\"></a>4.[^](#admonish): Also refs should be important to the background, not some randomly chosen vaguely related stuff. Include a web link if possible in refs as above.<br>\n",
    "<a name=\"sotanote\"></a>5.[^](#sota): Perhaps the current state of the art solution such as you see on [Papers with code](https://paperswithcode.com/sota). Or maybe not SOTA, but rather a standard textbook/Kaggle solution to this kind of problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
